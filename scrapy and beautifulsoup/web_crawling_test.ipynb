{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9E0lcuHdJBHJ",
        "outputId": "f92999d9-b12e-43d8-931e-74b5715a12ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting scrapy\n",
            "  Downloading Scrapy-2.6.2-py2.py3-none-any.whl (264 kB)\n",
            "\u001b[K     |████████████████████████████████| 264 kB 4.1 MB/s \n",
            "\u001b[?25hCollecting itemadapter>=0.1.0\n",
            "  Downloading itemadapter-0.7.0-py3-none-any.whl (10 kB)\n",
            "Collecting queuelib>=1.4.2\n",
            "  Downloading queuelib-1.6.2-py2.py3-none-any.whl (13 kB)\n",
            "Collecting cssselect>=0.9.1\n",
            "  Downloading cssselect-1.1.0-py2.py3-none-any.whl (16 kB)\n",
            "Collecting service-identity>=16.0.0\n",
            "  Downloading service_identity-21.1.0-py2.py3-none-any.whl (12 kB)\n",
            "Collecting Twisted>=17.9.0\n",
            "  Downloading Twisted-22.8.0-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 59.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=3.5.0 in /usr/local/lib/python3.7/dist-packages (from scrapy) (4.9.1)\n",
            "Collecting parsel>=1.5.0\n",
            "  Downloading parsel-1.6.0-py2.py3-none-any.whl (13 kB)\n",
            "Collecting cryptography>=2.0\n",
            "  Downloading cryptography-38.0.1-cp36-abi3-manylinux_2_24_x86_64.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 40.1 MB/s \n",
            "\u001b[?25hCollecting protego>=0.1.15\n",
            "  Downloading Protego-0.2.1-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting zope.interface>=4.1.3\n",
            "  Downloading zope.interface-5.4.0-cp37-cp37m-manylinux2010_x86_64.whl (251 kB)\n",
            "\u001b[K     |████████████████████████████████| 251 kB 54.0 MB/s \n",
            "\u001b[?25hCollecting itemloaders>=1.0.1\n",
            "  Downloading itemloaders-1.0.6-py3-none-any.whl (11 kB)\n",
            "Collecting PyDispatcher>=2.0.5\n",
            "  Downloading PyDispatcher-2.0.6.tar.gz (38 kB)\n",
            "Collecting w3lib>=1.17.0\n",
            "  Downloading w3lib-2.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting pyOpenSSL>=16.2.0\n",
            "  Downloading pyOpenSSL-22.1.0-py3-none-any.whl (57 kB)\n",
            "\u001b[K     |████████████████████████████████| 57 kB 4.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from scrapy) (57.4.0)\n",
            "Collecting tldextract\n",
            "  Downloading tldextract-3.3.1-py3-none-any.whl (93 kB)\n",
            "\u001b[K     |████████████████████████████████| 93 kB 2.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=2.0->scrapy) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=2.0->scrapy) (2.21)\n",
            "Collecting jmespath>=0.9.5\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: six>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from parsel>=1.5.0->scrapy) (1.15.0)\n",
            "Requirement already satisfied: pyasn1 in /usr/local/lib/python3.7/dist-packages (from service-identity>=16.0.0->scrapy) (0.4.8)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.7/dist-packages (from service-identity>=16.0.0->scrapy) (22.1.0)\n",
            "Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.7/dist-packages (from service-identity>=16.0.0->scrapy) (0.2.8)\n",
            "Collecting hyperlink>=17.1.1\n",
            "  Downloading hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n",
            "\u001b[K     |████████████████████████████████| 74 kB 3.4 MB/s \n",
            "\u001b[?25hCollecting constantly>=15.1\n",
            "  Downloading constantly-15.1.0-py2.py3-none-any.whl (7.9 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.6.5 in /usr/local/lib/python3.7/dist-packages (from Twisted>=17.9.0->scrapy) (4.1.1)\n",
            "Collecting Automat>=0.8.0\n",
            "  Downloading Automat-20.2.0-py2.py3-none-any.whl (31 kB)\n",
            "Collecting incremental>=21.3.0\n",
            "  Downloading incremental-21.3.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: idna>=2.5 in /usr/local/lib/python3.7/dist-packages (from hyperlink>=17.1.1->Twisted>=17.9.0->scrapy) (2.10)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from tldextract->scrapy) (2.23.0)\n",
            "Collecting requests-file>=1.4\n",
            "  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from tldextract->scrapy) (3.8.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->tldextract->scrapy) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->tldextract->scrapy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->tldextract->scrapy) (3.0.4)\n",
            "Building wheels for collected packages: PyDispatcher\n",
            "  Building wheel for PyDispatcher (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyDispatcher: filename=PyDispatcher-2.0.6-py3-none-any.whl size=11958 sha256=125d0f5e02497462730b0c56b43bbe3a30e2599f9e982496de078eedb73a4879\n",
            "  Stored in directory: /root/.cache/pip/wheels/c9/d6/6a/de198d890277cde60ca3dbebe7ae592d3b381c7d9bb2455f4d\n",
            "Successfully built PyDispatcher\n",
            "Installing collected packages: w3lib, cssselect, zope.interface, requests-file, parsel, jmespath, itemadapter, incremental, hyperlink, cryptography, constantly, Automat, Twisted, tldextract, service-identity, queuelib, pyOpenSSL, PyDispatcher, protego, itemloaders, scrapy\n",
            "Successfully installed Automat-20.2.0 PyDispatcher-2.0.6 Twisted-22.8.0 constantly-15.1.0 cryptography-38.0.1 cssselect-1.1.0 hyperlink-21.0.0 incremental-21.3.0 itemadapter-0.7.0 itemloaders-1.0.6 jmespath-1.0.1 parsel-1.6.0 protego-0.2.1 pyOpenSSL-22.1.0 queuelib-1.6.2 requests-file-1.5.1 scrapy-2.6.2 service-identity-21.1.0 tldextract-3.3.1 w3lib-2.0.1 zope.interface-5.4.0\n"
          ]
        }
      ],
      "source": [
        "! pip install scrapy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! scrapy startproject quotescrawler"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBvd53nUJRCq",
        "outputId": "aaa77ae2-0f2b-49f0-a999-3200d5852b1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New Scrapy project 'quotescrawler', using template directory '/usr/local/lib/python3.7/dist-packages/scrapy/templates/project', created in:\n",
            "    /content/quotescrawler\n",
            "\n",
            "You can start your first spider with:\n",
            "    cd quotescrawler\n",
            "    scrapy genspider example example.com\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "! scrapy crawl quotes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VjI2D6LHK6To",
        "outputId": "2bf893a9-e5c8-4594-a309-9c5dc5fb5f42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: scrapy: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/quotescrawler/quotescrawler"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJqMizqeLdKr",
        "outputId": "6ce4e029-53f7-4c61-b449-9e6ff5a9b5bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/quotescrawler/quotescrawler\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! scrapy crawl quotes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_KG5DHwLhra",
        "outputId": "6bbc9c9a-7bd5-4d87-fd62-0850521cdbf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-09-25 15:05:19 [scrapy.utils.log] INFO: Scrapy 2.6.2 started (bot: quotescrawler)\n",
            "2022-09-25 15:05:19 [scrapy.utils.log] INFO: Versions: lxml 4.9.1.0, libxml2 2.9.14, cssselect 1.1.0, parsel 1.6.0, w3lib 2.0.1, Twisted 22.8.0, Python 3.7.14 (default, Sep  8 2022, 00:06:44) - [GCC 7.5.0], pyOpenSSL 22.0.0 (OpenSSL 3.0.5 5 Jul 2022), cryptography 38.0.1, Platform Linux-5.10.133+-x86_64-with-Ubuntu-18.04-bionic\n",
            "2022-09-25 15:05:19 [scrapy.crawler] INFO: Overridden settings:\n",
            "{'BOT_NAME': 'quotescrawler',\n",
            " 'NEWSPIDER_MODULE': 'quotescrawler.spiders',\n",
            " 'ROBOTSTXT_OBEY': True,\n",
            " 'SPIDER_MODULES': ['quotescrawler.spiders']}\n",
            "2022-09-25 15:05:19 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.epollreactor.EPollReactor\n",
            "2022-09-25 15:05:19 [scrapy.extensions.telnet] INFO: Telnet Password: 2b9ebdf6ca1a1ecc\n",
            "2022-09-25 15:05:19 [scrapy.middleware] INFO: Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.memusage.MemoryUsage',\n",
            " 'scrapy.extensions.logstats.LogStats']\n",
            "2022-09-25 15:05:19 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
            "['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
            " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
            " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
            " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
            " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
            " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
            " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
            "2022-09-25 15:05:19 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
            "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
            " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
            " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
            " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
            " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
            "2022-09-25 15:05:19 [scrapy.middleware] INFO: Enabled item pipelines:\n",
            "[]\n",
            "2022-09-25 15:05:19 [scrapy.core.engine] INFO: Spider opened\n",
            "2022-09-25 15:05:19 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
            "2022-09-25 15:05:19 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
            "2022-09-25 15:05:19 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://quotes.toscrape.com/robots.txt> (referer: None)\n",
            "2022-09-25 15:05:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://quotes.toscrape.com/page/1/> (referer: None)\n",
            "2022-09-25 15:05:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://quotes.toscrape.com/page/2/> (referer: None)\n",
            "2022-09-25 15:05:19 [scrapy.core.scraper] ERROR: Spider error processing <GET https://quotes.toscrape.com/page/1/> (referer: None)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/twisted/internet/defer.py\", line 892, in _runCallbacks\n",
            "    current.result, *args, **kwargs\n",
            "  File \"/content/quotescrawler/quotescrawler/spiders/quotes_spider.py\", line 17, in parse\n",
            "    with open(filename,'wb') as f:\n",
            "TypeError: expected str, bytes or os.PathLike object, not tuple\n",
            "2022-09-25 15:05:19 [scrapy.core.scraper] ERROR: Spider error processing <GET https://quotes.toscrape.com/page/2/> (referer: None)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/twisted/internet/defer.py\", line 892, in _runCallbacks\n",
            "    current.result, *args, **kwargs\n",
            "  File \"/content/quotescrawler/quotescrawler/spiders/quotes_spider.py\", line 17, in parse\n",
            "    with open(filename,'wb') as f:\n",
            "TypeError: expected str, bytes or os.PathLike object, not tuple\n",
            "2022-09-25 15:05:20 [scrapy.core.engine] INFO: Closing spider (finished)\n",
            "2022-09-25 15:05:20 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
            "{'downloader/request_bytes': 681,\n",
            " 'downloader/request_count': 3,\n",
            " 'downloader/request_method_count/GET': 3,\n",
            " 'downloader/response_bytes': 25579,\n",
            " 'downloader/response_count': 3,\n",
            " 'downloader/response_status_count/200': 2,\n",
            " 'downloader/response_status_count/404': 1,\n",
            " 'elapsed_time_seconds': 0.336181,\n",
            " 'finish_reason': 'finished',\n",
            " 'finish_time': datetime.datetime(2022, 9, 25, 15, 5, 20, 37086),\n",
            " 'log_count/DEBUG': 4,\n",
            " 'log_count/ERROR': 2,\n",
            " 'log_count/INFO': 10,\n",
            " 'memusage/max': 90013696,\n",
            " 'memusage/startup': 90013696,\n",
            " 'response_received_count': 3,\n",
            " 'robotstxt/request_count': 1,\n",
            " 'robotstxt/response_count': 1,\n",
            " 'robotstxt/response_status_count/404': 1,\n",
            " 'scheduler/dequeued': 2,\n",
            " 'scheduler/dequeued/memory': 2,\n",
            " 'scheduler/enqueued': 2,\n",
            " 'scheduler/enqueued/memory': 2,\n",
            " 'spider_exceptions/TypeError': 2,\n",
            " 'start_time': datetime.datetime(2022, 9, 25, 15, 5, 19, 700905)}\n",
            "2022-09-25 15:05:20 [scrapy.core.engine] INFO: Spider closed (finished)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! scrapy crawl quotes\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kdYx_69VLsCG",
        "outputId": "284423dd-e276-4273-bbee-16d541ca7f16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-09-25 15:12:28 [scrapy.utils.log] INFO: Scrapy 2.6.2 started (bot: quotescrawler)\n",
            "2022-09-25 15:12:28 [scrapy.utils.log] INFO: Versions: lxml 4.9.1.0, libxml2 2.9.14, cssselect 1.1.0, parsel 1.6.0, w3lib 2.0.1, Twisted 22.8.0, Python 3.7.14 (default, Sep  8 2022, 00:06:44) - [GCC 7.5.0], pyOpenSSL 22.0.0 (OpenSSL 3.0.5 5 Jul 2022), cryptography 38.0.1, Platform Linux-5.10.133+-x86_64-with-Ubuntu-18.04-bionic\n",
            "2022-09-25 15:12:28 [scrapy.crawler] INFO: Overridden settings:\n",
            "{'BOT_NAME': 'quotescrawler',\n",
            " 'NEWSPIDER_MODULE': 'quotescrawler.spiders',\n",
            " 'ROBOTSTXT_OBEY': True,\n",
            " 'SPIDER_MODULES': ['quotescrawler.spiders']}\n",
            "2022-09-25 15:12:28 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.epollreactor.EPollReactor\n",
            "2022-09-25 15:12:28 [scrapy.extensions.telnet] INFO: Telnet Password: 3b1fc259295a9a4e\n",
            "2022-09-25 15:12:28 [scrapy.middleware] INFO: Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.memusage.MemoryUsage',\n",
            " 'scrapy.extensions.logstats.LogStats']\n",
            "2022-09-25 15:12:28 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
            "['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
            " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
            " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
            " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
            " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
            " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
            " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
            "2022-09-25 15:12:28 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
            "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
            " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
            " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
            " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
            " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
            "2022-09-25 15:12:28 [scrapy.middleware] INFO: Enabled item pipelines:\n",
            "[]\n",
            "2022-09-25 15:12:28 [scrapy.core.engine] INFO: Spider opened\n",
            "2022-09-25 15:12:28 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
            "2022-09-25 15:12:28 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
            "2022-09-25 15:12:28 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://quotes.toscrape.com/robots.txt> (referer: None)\n",
            "2022-09-25 15:12:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://quotes.toscrape.com/page/1/> (referer: None)\n",
            "2022-09-25 15:12:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://quotes.toscrape.com/page/2/> (referer: None)\n",
            "2022-09-25 15:12:28 [scrapy.core.engine] INFO: Closing spider (finished)\n",
            "2022-09-25 15:12:28 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
            "{'downloader/request_bytes': 681,\n",
            " 'downloader/request_count': 3,\n",
            " 'downloader/request_method_count/GET': 3,\n",
            " 'downloader/response_bytes': 25579,\n",
            " 'downloader/response_count': 3,\n",
            " 'downloader/response_status_count/200': 2,\n",
            " 'downloader/response_status_count/404': 1,\n",
            " 'elapsed_time_seconds': 0.285407,\n",
            " 'finish_reason': 'finished',\n",
            " 'finish_time': datetime.datetime(2022, 9, 25, 15, 12, 28, 746489),\n",
            " 'log_count/DEBUG': 4,\n",
            " 'log_count/INFO': 10,\n",
            " 'memusage/max': 90030080,\n",
            " 'memusage/startup': 90030080,\n",
            " 'response_received_count': 3,\n",
            " 'robotstxt/request_count': 1,\n",
            " 'robotstxt/response_count': 1,\n",
            " 'robotstxt/response_status_count/404': 1,\n",
            " 'scheduler/dequeued': 2,\n",
            " 'scheduler/dequeued/memory': 2,\n",
            " 'scheduler/enqueued': 2,\n",
            " 'scheduler/enqueued/memory': 2,\n",
            " 'start_time': datetime.datetime(2022, 9, 25, 15, 12, 28, 461082)}\n",
            "2022-09-25 15:12:28 [scrapy.core.engine] INFO: Spider closed (finished)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! scrapy crawl quotes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wH6BCS_nNe5n",
        "outputId": "ed0b3de7-4e9a-4ded-9422-d3f6d27106ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-09-25 15:14:00 [scrapy.utils.log] INFO: Scrapy 2.6.2 started (bot: quotescrawler)\n",
            "2022-09-25 15:14:00 [scrapy.utils.log] INFO: Versions: lxml 4.9.1.0, libxml2 2.9.14, cssselect 1.1.0, parsel 1.6.0, w3lib 2.0.1, Twisted 22.8.0, Python 3.7.14 (default, Sep  8 2022, 00:06:44) - [GCC 7.5.0], pyOpenSSL 22.0.0 (OpenSSL 3.0.5 5 Jul 2022), cryptography 38.0.1, Platform Linux-5.10.133+-x86_64-with-Ubuntu-18.04-bionic\n",
            "2022-09-25 15:14:00 [scrapy.crawler] INFO: Overridden settings:\n",
            "{'BOT_NAME': 'quotescrawler',\n",
            " 'NEWSPIDER_MODULE': 'quotescrawler.spiders',\n",
            " 'ROBOTSTXT_OBEY': True,\n",
            " 'SPIDER_MODULES': ['quotescrawler.spiders']}\n",
            "2022-09-25 15:14:00 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.epollreactor.EPollReactor\n",
            "2022-09-25 15:14:00 [scrapy.extensions.telnet] INFO: Telnet Password: 1e457a7f2a567ff0\n",
            "2022-09-25 15:14:00 [scrapy.middleware] INFO: Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.memusage.MemoryUsage',\n",
            " 'scrapy.extensions.logstats.LogStats']\n",
            "2022-09-25 15:14:00 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
            "['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
            " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
            " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
            " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
            " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
            " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
            " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
            "2022-09-25 15:14:00 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
            "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
            " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
            " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
            " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
            " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
            "2022-09-25 15:14:00 [scrapy.middleware] INFO: Enabled item pipelines:\n",
            "[]\n",
            "2022-09-25 15:14:00 [scrapy.core.engine] INFO: Spider opened\n",
            "2022-09-25 15:14:00 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
            "2022-09-25 15:14:00 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
            "2022-09-25 15:14:00 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://quotes.toscrape.com/robots.txt> (referer: None)\n",
            "2022-09-25 15:14:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://quotes.toscrape.com/page/1/> (referer: None)\n",
            "2022-09-25 15:14:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://quotes.toscrape.com/page/2/> (referer: None)\n",
            "2022-09-25 15:14:00 [scrapy.core.engine] INFO: Closing spider (finished)\n",
            "2022-09-25 15:14:00 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
            "{'downloader/request_bytes': 681,\n",
            " 'downloader/request_count': 3,\n",
            " 'downloader/request_method_count/GET': 3,\n",
            " 'downloader/response_bytes': 25579,\n",
            " 'downloader/response_count': 3,\n",
            " 'downloader/response_status_count/200': 2,\n",
            " 'downloader/response_status_count/404': 1,\n",
            " 'elapsed_time_seconds': 0.207338,\n",
            " 'finish_reason': 'finished',\n",
            " 'finish_time': datetime.datetime(2022, 9, 25, 15, 14, 0, 479566),\n",
            " 'log_count/DEBUG': 4,\n",
            " 'log_count/INFO': 10,\n",
            " 'memusage/max': 90030080,\n",
            " 'memusage/startup': 90030080,\n",
            " 'response_received_count': 3,\n",
            " 'robotstxt/request_count': 1,\n",
            " 'robotstxt/response_count': 1,\n",
            " 'robotstxt/response_status_count/404': 1,\n",
            " 'scheduler/dequeued': 2,\n",
            " 'scheduler/dequeued/memory': 2,\n",
            " 'scheduler/enqueued': 2,\n",
            " 'scheduler/enqueued/memory': 2,\n",
            " 'start_time': datetime.datetime(2022, 9, 25, 15, 14, 0, 272228)}\n",
            "2022-09-25 15:14:00 [scrapy.core.engine] INFO: Spider closed (finished)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!scrapy crawl spy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lu__Kc2yOjAQ",
        "outputId": "8a759813-b421-418f-fcb0-6d583eba37b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-09-25 15:18:45 [scrapy.utils.log] INFO: Scrapy 2.6.2 started (bot: quotescrawler)\n",
            "2022-09-25 15:18:45 [scrapy.utils.log] INFO: Versions: lxml 4.9.1.0, libxml2 2.9.14, cssselect 1.1.0, parsel 1.6.0, w3lib 2.0.1, Twisted 22.8.0, Python 3.7.14 (default, Sep  8 2022, 00:06:44) - [GCC 7.5.0], pyOpenSSL 22.0.0 (OpenSSL 3.0.5 5 Jul 2022), cryptography 38.0.1, Platform Linux-5.10.133+-x86_64-with-Ubuntu-18.04-bionic\n",
            "2022-09-25 15:18:45 [scrapy.crawler] INFO: Overridden settings:\n",
            "{'BOT_NAME': 'quotescrawler',\n",
            " 'NEWSPIDER_MODULE': 'quotescrawler.spiders',\n",
            " 'ROBOTSTXT_OBEY': True,\n",
            " 'SPIDER_MODULES': ['quotescrawler.spiders']}\n",
            "2022-09-25 15:18:45 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.epollreactor.EPollReactor\n",
            "2022-09-25 15:18:45 [scrapy.extensions.telnet] INFO: Telnet Password: ee9d1eabe79f3eed\n",
            "2022-09-25 15:18:45 [scrapy.middleware] INFO: Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.memusage.MemoryUsage',\n",
            " 'scrapy.extensions.logstats.LogStats']\n",
            "2022-09-25 15:18:45 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
            "['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
            " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
            " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
            " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
            " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
            " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
            " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
            "2022-09-25 15:18:45 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
            "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
            " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
            " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
            " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
            " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
            "2022-09-25 15:18:45 [scrapy.middleware] INFO: Enabled item pipelines:\n",
            "[]\n",
            "2022-09-25 15:18:45 [scrapy.core.engine] INFO: Spider opened\n",
            "2022-09-25 15:18:45 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
            "2022-09-25 15:18:45 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
            "2022-09-25 15:18:45 [filelock] DEBUG: Attempting to acquire lock 139776852246736 on /root/.cache/python-tldextract/3.7.14.final__usr__7d8fdf__tldextract-3.3.1/publicsuffix.org-tlds/de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n",
            "2022-09-25 15:18:45 [filelock] DEBUG: Lock 139776852246736 acquired on /root/.cache/python-tldextract/3.7.14.final__usr__7d8fdf__tldextract-3.3.1/publicsuffix.org-tlds/de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n",
            "2022-09-25 15:18:45 [filelock] DEBUG: Attempting to acquire lock 139776852249040 on /root/.cache/python-tldextract/3.7.14.final__usr__7d8fdf__tldextract-3.3.1/urls/62bf135d1c2f3d4db4228b9ecaf507a2.tldextract.json.lock\n",
            "2022-09-25 15:18:45 [filelock] DEBUG: Lock 139776852249040 acquired on /root/.cache/python-tldextract/3.7.14.final__usr__7d8fdf__tldextract-3.3.1/urls/62bf135d1c2f3d4db4228b9ecaf507a2.tldextract.json.lock\n",
            "2022-09-25 15:18:45 [urllib3.connectionpool] DEBUG: Starting new HTTPS connection (1): publicsuffix.org:443\n",
            "2022-09-25 15:18:46 [urllib3.connectionpool] DEBUG: https://publicsuffix.org:443 \"GET /list/public_suffix_list.dat HTTP/1.1\" 200 None\n",
            "2022-09-25 15:18:46 [filelock] DEBUG: Attempting to release lock 139776852249040 on /root/.cache/python-tldextract/3.7.14.final__usr__7d8fdf__tldextract-3.3.1/urls/62bf135d1c2f3d4db4228b9ecaf507a2.tldextract.json.lock\n",
            "2022-09-25 15:18:46 [filelock] DEBUG: Lock 139776852249040 released on /root/.cache/python-tldextract/3.7.14.final__usr__7d8fdf__tldextract-3.3.1/urls/62bf135d1c2f3d4db4228b9ecaf507a2.tldextract.json.lock\n",
            "2022-09-25 15:18:46 [filelock] DEBUG: Attempting to release lock 139776852246736 on /root/.cache/python-tldextract/3.7.14.final__usr__7d8fdf__tldextract-3.3.1/publicsuffix.org-tlds/de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n",
            "2022-09-25 15:18:46 [filelock] DEBUG: Lock 139776852246736 released on /root/.cache/python-tldextract/3.7.14.final__usr__7d8fdf__tldextract-3.3.1/publicsuffix.org-tlds/de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n",
            "2022-09-25 15:18:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://in.search.yahoo.com/robots.txt> (referer: None)\n",
            "2022-09-25 15:18:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://in.search.yahoo.com/?fr2=inr> (referer: None)\n",
            "2022-09-25 15:18:46 [scrapy.core.scraper] ERROR: Spider error processing <GET https://in.search.yahoo.com/?fr2=inr> (referer: None)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/twisted/internet/defer.py\", line 892, in _runCallbacks\n",
            "    current.result, *args, **kwargs\n",
            "  File \"/content/quotescrawler/quotescrawler/spiders/yahoo_scraper.py\", line 17, in parse\n",
            "    with open(filename,'wb') as f:\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'page-https://in.search.yahoo.com/?fr2=inr.html'\n",
            "2022-09-25 15:18:46 [scrapy.core.engine] INFO: Closing spider (finished)\n",
            "2022-09-25 15:18:46 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
            "{'downloader/request_bytes': 767,\n",
            " 'downloader/request_count': 2,\n",
            " 'downloader/request_method_count/GET': 2,\n",
            " 'downloader/response_bytes': 22739,\n",
            " 'downloader/response_count': 2,\n",
            " 'downloader/response_status_count/200': 2,\n",
            " 'elapsed_time_seconds': 1.24992,\n",
            " 'finish_reason': 'finished',\n",
            " 'finish_time': datetime.datetime(2022, 9, 25, 15, 18, 46, 802760),\n",
            " 'httpcompression/response_bytes': 58596,\n",
            " 'httpcompression/response_count': 1,\n",
            " 'log_count/DEBUG': 13,\n",
            " 'log_count/ERROR': 1,\n",
            " 'log_count/INFO': 10,\n",
            " 'memusage/max': 90226688,\n",
            " 'memusage/startup': 90226688,\n",
            " 'response_received_count': 2,\n",
            " 'robotstxt/request_count': 1,\n",
            " 'robotstxt/response_count': 1,\n",
            " 'robotstxt/response_status_count/200': 1,\n",
            " 'scheduler/dequeued': 1,\n",
            " 'scheduler/dequeued/memory': 1,\n",
            " 'scheduler/enqueued': 1,\n",
            " 'scheduler/enqueued/memory': 1,\n",
            " 'spider_exceptions/FileNotFoundError': 1,\n",
            " 'start_time': datetime.datetime(2022, 9, 25, 15, 18, 45, 552840)}\n",
            "2022-09-25 15:18:46 [scrapy.core.engine] INFO: Spider closed (finished)\n"
          ]
        }
      ]
    }
  ]
}